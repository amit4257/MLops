# -*- coding: utf-8 -*-
"""Major 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q3vhksnrSioq-YTJLMJKDaajM_dIkkrj
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
data=pd.read_excel('/content/gdrive/MyDrive/company/Restaurant_Reviews.xlsx')
print(data.shape)
data.head()

data.info()

data.describe()

data.hist(bins = 30, figsize = (8,5), color = 'r')

import seaborn as sns
sns.countplot(data['Liked'],label='Count')

# Let's get the length of the messages
data['length']=data['Review'].apply(len)

data

data['length'].plot(bins=100, kind='hist')

positive = data[data['Liked']==0]

negative = data[data['Liked']==1]

sentences=data['Review'].tolist()
sentences

len(sentences)

sentences_as_one_string="".join(sentences)

import matplotlib.pyplot as plt
!pip install WordCloud
from wordcloud import WordCloud

plt.figure(figsize=(13,10))
plt.imshow(WordCloud().generate(sentences_as_one_string))

negative_list = negative['Review'].tolist()
negative_list
negative_sentences_as_one_string = " ".join(negative_list)
plt.figure(figsize=(13,10))
plt.imshow(WordCloud().generate(negative_sentences_as_one_string))

positive_list = positive['Review'].tolist()
positive_list
positive_sentences_as_one_string = " ".join(positive_list)
plt.figure(figsize=(13,10))
plt.imshow(WordCloud().generate(positive_sentences_as_one_string))

import string
string.punctuation

import nltk # Natural Language tool kit 

nltk.download('stopwords')

# You have to download stopwords Package to execute this command
from nltk.corpus import stopwords
stopwords.words('english')

# Let's define a pipeline to clean up all the messages 
# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords

def message_cleaning(message):
    Test_punc_removed = [char for char in message if char not in string.punctuation]
    Test_punc_removed_join = ''.join(Test_punc_removed)
    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return Test_punc_removed_join_clean

# Let's test the newly added function
data_df_clean = data['Review'].apply(message_cleaning)

print(data_df_clean[5]) # show the cleaned up version

from sklearn.feature_extraction.text import CountVectorizer
# Define the cleaning pipeline we defined earlier
vectorizer = CountVectorizer(analyzer = message_cleaning)
data_countvectorizer = vectorizer.fit_transform(data['Review'])

print(vectorizer.get_feature_names())

print(data_countvectorizer.toarray())

data_countvectorizer.shape

Restaurant = pd.DataFrame(data_countvectorizer.toarray())

X = Restaurant

X

y = data['Liked']

X.shape

y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
NB_classifier.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix

# Predicting the Test set results
y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))

